# -*- coding: utf-8 -*-
"""Phishing_RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqoQKdjAN3Wuw6HoJOOVLukorsHqXA8N
"""

import random
import numpy as np
import matplotlib.pyplot as plt

# Tracking Q-values and performance metrics
q_values_over_time = []
correct_classifications_over_time = []

# Define a placeholder environment for Phishing Detection
class PhishingEnv:
    def __init__(self):
        self.action_space = ["not_phishing", "phishing"]  # Possible actions
        self.state_space = 2  # Placeholder for state space (e.g., phishing or not)

    def reset(self):
        # Reset to an initial state (could be a random URL for example)
        self.current_state = random.choice(["phishing", "not_phishing"])
        return self.current_state

    def step(self, action):
        # Check if the action matches the current state
        correct_action = "phishing" if self.current_state == "phishing" else "not_phishing"
        reward = 1 if action == correct_action else -1
        done = True  # End of episode after one action in this example

        # Transition to a new state (reset for simplicity)
        next_state = random.choice(["phishing", "not_phishing"])
        return next_state, reward, done

# Q-learning agent modified to track Q-value updates
class QLearningAgent:
    def __init__(self, actions):
        self.actions = actions
        self.q_table = {}

    def get_action(self, state):
        # Epsilon-greedy policy
        if state not in self.q_table:
            self.q_table[state] = {action: 0 for action in self.actions}
        if random.uniform(0, 1) < 0.1:  # Explore with 0.1 probability
            return random.choice(self.actions)
        else:
            return max(self.q_table[state], key=self.q_table[state].get)

    def update_q_value(self, state, action, reward, next_state):
        # Initialize Q-values for unseen states
        if state not in self.q_table:
            self.q_table[state] = {action: 0 for action in self.actions}
        if next_state not in self.q_table:
            self.q_table[next_state] = {action: 0 for action in self.actions}

        # Update the Q-value using the Q-learning formula
        self.q_table[state][action] += 0.1 * (reward + 0.9 * max(self.q_table[next_state].values()) - self.q_table[state][action])

        # Track average Q-values for convergence plot
        avg_q_value = np.mean([q for actions in self.q_table.values() for q in actions.values()])
        q_values_over_time.append(avg_q_value)

# Main function to train the agent and generate visualizations
def main():
    global q_values_over_time, correct_classifications_over_time
    env = PhishingEnv()
    agent = QLearningAgent(env.action_space)

    # Training loop
    correct_classifications = 0  # Track cumulative correct classifications

    for episode in range(1000):
        state = env.reset()
        done = False
        episode_correct_classifications = 0  # Track correct classifications per episode

        while not done:
            action = agent.get_action(str(state))
            next_state, reward, done = env.step(action)
            agent.update_q_value(str(state), action, reward, str(next_state))
            state = next_state

            if reward > 0:  # Correct classification
                correct_classifications += 1
                episode_correct_classifications += 1

        # Track correct classifications after each episode
        correct_classifications_over_time.append(correct_classifications)

        # Print performance every 100 episodes
        if episode % 100 == 0:
            print(f"Episode: {episode}, Correct Classifications So Far: {correct_classifications}")

    # Visualize results
    plot_q_value_convergence(q_values_over_time)
    plot_training_performance(correct_classifications_over_time)

# Plot average Q-values over time to visualize convergence
def plot_q_value_convergence(q_values_over_time):
    plt.figure(figsize=(10, 5))
    plt.plot(q_values_over_time, color="purple", label="Average Q-value")
    plt.xlabel("Training Steps")
    plt.ylabel("Average Q-value")
    plt.title("Q-Value Convergence Over Time")
    plt.legend()
    plt.show()

# Plot correct classifications over episodes to visualize training performance
def plot_training_performance(correct_classifications_over_time):
    plt.figure(figsize=(10, 5))
    plt.plot(correct_classifications_over_time, color="green", label="Cumulative Correct Classifications")
    plt.xlabel("Episodes")
    plt.ylabel("Correct Classifications")
    plt.title("Training Performance: Correct Classifications Over Episodes")
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()